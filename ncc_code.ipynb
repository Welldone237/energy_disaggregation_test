{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbf4ea08-879a-4d18-bb22-b3c7d43a6d53",
   "metadata": {},
   "source": [
    "Compute the normalized cross correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01db01dd-720c-49ea-8b6e-74cb9342e375",
   "metadata": {},
   "source": [
    "Dans la case qui suit j'ai écrit le code en supposant que les données venaient d'un dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490b07d6-bca4-40db-a441-f33de0f9bfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calcul corrélation croisée normalisée entre les données d'appareils et une fonction pour regrouper\n",
    "les appareils corrélés en \"super appareils\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Import librairies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def compute_ncc(signal1, signal2):\n",
    "    \"\"\"Compute Normalized Cross-Correlation between two signals\"\"\"\n",
    "    mean1 = np.mean(signal1)\n",
    "    mean2 = np.mean(signal2)\n",
    "    signal1_centered = signal1 - mean1\n",
    "    signal2_centered = signal2 - mean2\n",
    "    ncc = np.sum(signal1_centered * signal2_centered) / (np.linalg.norm(signal1_centered) * np.linalg.norm(signal2_centered))\n",
    "    return ncc\n",
    "\n",
    "def extend_to_same_length(signal1, signal2):\n",
    "    \"\"\"Extend the shorter signal by repeating its mean value until both signals have the same length\"\"\"\n",
    "    len1, len2 = len(signal1), len(signal2)\n",
    "    if len1 == len2:\n",
    "        return signal1, signal2\n",
    "    \n",
    "    if len1 < len2:\n",
    "        mean_val = np.mean(signal1)\n",
    "        extended_signal = np.pad(signal1, (0, len2 - len1), 'constant', constant_values=(mean_val,))\n",
    "        return extended_signal, signal2\n",
    "    else:\n",
    "        mean_val = np.mean(signal2)\n",
    "        extended_signal = np.pad(signal2, (0, len1 - len2), 'constant', constant_values=(mean_val,))\n",
    "        return signal1, extended_signal\n",
    "\n",
    "def cluster_devices(device_signals, ncc_threshold):\n",
    "    \"\"\"Cluster devices into super device groups based on NCC\"\"\"\n",
    "    device_graph = {}\n",
    "    for i, sig1 in enumerate(device_signals):\n",
    "        for j, sig2 in enumerate(device_signals):\n",
    "            if i != j:\n",
    "                sig1_ext, sig2_ext = extend_to_same_length(sig1, sig2)\n",
    "                ncc = compute_ncc(sig1_ext, sig2_ext)\n",
    "                if ncc >= ncc_threshold:\n",
    "                    device_graph.setdefault(i, []).append(j)\n",
    "                    device_graph.setdefault(j, []).append(i)\n",
    "    \n",
    "    super_device_groups = []\n",
    "    visited = set()\n",
    "    for node in device_graph:\n",
    "        if node not in visited:\n",
    "            group = [node]\n",
    "            queue = [node]\n",
    "            visited.add(node)\n",
    "            while queue:\n",
    "                curr = queue.pop(0)\n",
    "                neighbors = device_graph.get(curr, [])\n",
    "                for neighbor in neighbors:\n",
    "                    if neighbor not in visited:\n",
    "                        group.append(neighbor)\n",
    "                        queue.append(neighbor)\n",
    "                        visited.add(neighbor)\n",
    "            super_device_groups.append(group)\n",
    "    \n",
    "    # Add isolated nodes\n",
    "    all_nodes = set(range(len(device_signals)))\n",
    "    isolated_nodes = all_nodes - set(visited)\n",
    "    for node in isolated_nodes:\n",
    "        super_device_groups.append([node])\n",
    "    \n",
    "    return super_device_groups\n",
    "\n",
    "# Reading data from the CSV file\n",
    "def read_device_signals(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    device_signals = [df[col].dropna().values for col in df.columns if col != 'number_of_states']\n",
    "    return device_signals\n",
    "\n",
    "# Utilisation\n",
    "csv_file = 'device_signals.csv'  \n",
    "ncc_threshold = 0.8\n",
    "\n",
    "device_signals = read_device_signals(csv_file)\n",
    "super_device_groups = cluster_devices(device_signals, ncc_threshold)\n",
    "print(\"Super device groups:\", super_device_groups)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b77612d-4432-4317-b033-e7b8554e73e3",
   "metadata": {},
   "source": [
    "Dans ce cas de figure ci, j'ai utilisé des données aléatoires pour exécuter le code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ac32a0c9-a9b5-4046-b3f6-70dfdbbd38ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Super device groups: [[0, 2], [1, 4], [3]]\n",
      "====================================================\n",
      "         device1  device2  device3  device4  device5\n",
      "device1      NaN   -0.600    1.000   -1.000   -0.216\n",
      "device2   -0.600      NaN   -0.600    0.600    0.893\n",
      "device3    1.000   -0.600      NaN   -1.000   -0.216\n",
      "device4   -1.000    0.600   -1.000      NaN    0.216\n",
      "device5   -0.216    0.893   -0.216    0.216      NaN\n",
      "====================================================\n",
      "NCC results have been saved to: results_ncc.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Calcul corrélation croisée normalisée entre les données d'appareils et une fonction pour regrouper\n",
    "les appareils corrélés en \"super appareils\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Import librairies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def compute_ncc(signal1, signal2):\n",
    "    \"\"\"Compute Normalized Cross-Correlation between two signals\"\"\"\n",
    "    mean1 = np.mean(signal1)\n",
    "    mean2 = np.mean(signal2)\n",
    "    signal1_centered = signal1 - mean1\n",
    "    signal2_centered = signal2 - mean2\n",
    "    ncc = np.sum(signal1_centered * signal2_centered) / (np.linalg.norm(signal1_centered) * np.linalg.norm(signal2_centered))\n",
    "    return ncc\n",
    "\n",
    "def extend_to_same_length(signal1, signal2):\n",
    "    \"\"\"Extend the shorter signal by repeating its mean value until both signals have the same length\"\"\"\n",
    "    len1, len2 = len(signal1), len(signal2)\n",
    "    if len1 == len2:\n",
    "        return signal1, signal2\n",
    "    \n",
    "    if len1 < len2:\n",
    "        mean_val = np.mean(signal1)\n",
    "        extended_signal = np.pad(signal1, (0, len2 - len1), 'constant', constant_values=(mean_val,))\n",
    "        return extended_signal, signal2\n",
    "    else:\n",
    "        mean_val = np.mean(signal2)\n",
    "        extended_signal = np.pad(signal2, (0, len1 - len2), 'constant', constant_values=(mean_val,))\n",
    "        return signal1, extended_signal\n",
    "    \n",
    "def cluster_devices(device_signals, ncc_threshold):\n",
    "    \"\"\"Cluster devices into super device groups based on NCC\"\"\"\n",
    "    super_device_groups = []\n",
    "    visited = set()\n",
    "    \n",
    "    for i, sig1 in enumerate(device_signals):\n",
    "        if i not in visited:\n",
    "            for j, sig2 in enumerate(device_signals):\n",
    "                if i != j and j not in visited:\n",
    "                    ncc = compute_ncc(sig1, sig2)\n",
    "                    if ncc > ncc_threshold:\n",
    "                        super_device_groups.append([i, j])\n",
    "                        visited.add(i)\n",
    "                        visited.add(j)\n",
    "                        break\n",
    "            else:\n",
    "                # Si aucun appareil n'est suffisamment corrélé, ajouter l'appareil seul\n",
    "                if i not in visited:\n",
    "                    super_device_groups.append([i])\n",
    "                    visited.add(i)\n",
    "    \n",
    "    return super_device_groups\n",
    "\n",
    "# Using\n",
    "device_signals = [\n",
    "    [0.1, 0.2, 0.3, 0.4],\n",
    "    [0.3, 0.4, 0.1, 0.2],\n",
    "    [0.5, 0.6, 0.7, 0.8],\n",
    "    [0.4, 0.3, 0.2, 0.1],\n",
    "    [0.8, 1.5, 0.5, 0.9]\n",
    "]\n",
    "ncc_threshold = 0.8\n",
    "super_device_groups = cluster_devices(device_signals, ncc_threshold)\n",
    "print(\"Super device groups:\", super_device_groups)\n",
    "\n",
    "print(\"=\" *52)\n",
    "   \n",
    "device_names = [\"device1\", \"device2\", \"device3\", \"device4\", \"device5\"]\n",
    "\n",
    "# Calcul de la matrice NCC\n",
    "ncc_matrix = compute_ncc_matrix(device_signals)\n",
    "\n",
    "# Stockage dans un DataFrame\n",
    "df_ncc = pd.DataFrame(ncc_matrix, index=device_names, columns=device_names)\n",
    "\n",
    "# Affichage du DataFrame\n",
    "print(df_ncc.round(3))\n",
    "\n",
    "# Sauvegarde dans un fichier CSV\n",
    "output_csv = 'results_ncc.csv'\n",
    "df_ncc.to_csv(output_csv, float_format='%.3f', na_rep='NaN')\n",
    "\n",
    "print(\"=\" *52)\n",
    "\n",
    "print(\"NCC results have been saved to:\", output_csv)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03690503-6ad0-42bf-aae7-1f0b2b92bbeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc6c1e4-5c97-484d-8c74-194a16300d91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d8f7fe-bc9e-48f6-b399-acde12a6f447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f5de59-7584-42d4-bbe2-4d0c29edf708",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edcd511-135b-49f3-96ce-b6f5b9c92841",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad216ee-15f3-4fdc-a536-31ac3cc38950",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83d19f4-bbb2-43e1-8975-28a9c4173ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943c5832-b307-4a97-a0fa-11abc8a985e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5fa771-b1b6-4450-ae21-54373d4ec479",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50423269-3594-4f4b-a312-dcfae1f47a65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3c7684-e021-4b65-a513-e5c2ce46a504",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01daef9e-d8ce-402b-9238-aa3b1e1b2b43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3304550c-3be4-4697-b379-e3fcf93410e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
